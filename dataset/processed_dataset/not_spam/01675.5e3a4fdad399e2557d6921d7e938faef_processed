so
then
tim
peter
is
all
like
tim
my
test
train
on
about
number
number
msg
and
a
binari
pickl
of
the
databas
is
approach
number
million
byte
that
shrink
to
under
number
million
byte
though
if
i
delet
all
the
wordinfo
record
with
spamprob
exactli
equal
to
unknown
spamprob
such
record
aren
t
need
when
score
an
unknown
word
get
a
made
up
probabl
of
unknown
spamprob
such
record
are
onli
need
for
train
i
ve
note
befor
that
a
score
onli
databas
can
be
leaner
that
s
pretti
good
i
wonder
how
much
better
you
could
do
by
use
some
custom
pickler
i
just
check
my
littl
dbm
file
and
found
a
lot
of
what
i
would
call
bloat
import
anydbm
hammi
d
hammi
persistentgrahambay
ham
db
db
anydbm
open
ham
db
db
neal
len
db
neal
ccopi
reg
n
reconstructor
nq
x
number
cclassifi
nwordinfo
nq
x
number
c
builtin
nobject
nq
x
number
ntrq
x
number
ga
xce
xbc
xfd
x
number
xbbok
x
number
k
x
number
k
x
number
g
xe
number
x
number
x
number
x
number
x
number
x
number
x
number
tb
number
d
wordinfo
neal
len
d
wordinfo
neal
wordinfo
number
number
number
number
number
number
number
number
ignor
the
fact
that
there
are
too
mani
zero
in
there
the
pickl
version
of
that
wordinfo
object
is
over
twice
as
larg
as
the
string
represent
so
we
could
get
a
number
decreas
in
size
just
by
use
the
string
represent
instead
of
the
pickl
right
someth
about
that
logic
seem
wrong
to
me
but
i
can
t
see
what
it
is
mayb
pickl
is
good
for
heterogen
data
type
but
everi
valu
of
our
big
dictionari
is
go
to
have
the
same
type
so
there
s
a
ton
of
redund
i
guess
that
explain
whi
it
compress
so
well
neal